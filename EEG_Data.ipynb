{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "#python3 -m pip install --upgrade example\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow import keras\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]C:\\Users\\Pichau\\AppData\\Local\\Temp\\ipykernel_2156\\1304392739.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  EEG_Data = EEG_Data.append(temp_df)\n",
      " 25%|██▌       | 1/4 [00:00<00:00,  7.62it/s]C:\\Users\\Pichau\\AppData\\Local\\Temp\\ipykernel_2156\\1304392739.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  EEG_Data = EEG_Data.append(temp_df)\n",
      " 50%|█████     | 2/4 [00:00<00:00,  8.60it/s]C:\\Users\\Pichau\\AppData\\Local\\Temp\\ipykernel_2156\\1304392739.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  EEG_Data = EEG_Data.append(temp_df)\n",
      " 75%|███████▌  | 3/4 [00:00<00:00,  7.91it/s]C:\\Users\\Pichau\\AppData\\Local\\Temp\\ipykernel_2156\\1304392739.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  EEG_Data = EEG_Data.append(temp_df)\n",
      "100%|██████████| 4/4 [00:00<00:00,  8.08it/s]\n"
     ]
    }
   ],
   "source": [
    "filenames_list = os.listdir('EEG_Data_handMovements') #lists all files on directory\n",
    "EEG_Data = pd.DataFrame({}) #creates empty df\n",
    "\n",
    "for file_name in tqdm(filenames_list): # adds each file from directory on temporary df\n",
    "    temp_df = pd.read_csv('EEG_Data_handMovements/'+file_name)\n",
    "    EEG_Data = EEG_Data.append(temp_df) \n",
    "\n",
    "EEG_Data.to_csv('users.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weighted average \n",
    "df_std = EEG_Data[EEG_Data.columns.drop(list(EEG_Data.filter(regex='m')))] #dropping all columns with mean values 'm'\n",
    "df_std.columns\n",
    "\n",
    "df_std.to_csv('weighted_avg.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean average \n",
    "df_m = EEG_Data[EEG_Data.columns.drop(list(EEG_Data.filter(regex='std')))]\n",
    "df_m.columns\n",
    "\n",
    "df_m.to_csv('mean_avg.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EEG_Data.head() #visualize\n",
    "EEG_Data.columns  \n",
    "\n",
    "all_columns=['Class','AF3 delta std','AF3 delta m','AF3 theta std','AF3 theta m','AF3 alpha std','AF3 alpha m','AF3 beta std','AF3 beta m','F7 delta std','F7 delta m','F7 theta std','F7 theta m','F7 alpha std','F7 alpha m','F7 beta std','F7 beta m','F3 delta std','F3 delta m','F3 theta std','F3 theta m','F3 alpha std','F3 alpha m','F3 beta std','F3 beta m','FC5 delta std','FC5 delta m','FC5 theta std','FC5 theta m','FC5 alpha std','FC5 alpha m','FC5 beta std','FC5 beta m','T7 delta std','T7 delta m','T7 theta std','T7 theta m','T7 alpha std','T7 alpha m','T7 beta std','T7 beta m','P7 delta std','P7 delta m','P7 theta std','P7 theta m','P7 alpha std','P7 alpha m','P7 beta std','P7 beta m','O1 delta std','O1 delta m','O1 theta std','O1 theta m','O1 alpha std','O1 alpha m','O1 beta std','O1 beta m','O2 delta std','O2 delta m','O2 theta std','O2 theta m','O2 alpha std','O2 alpha m','O2 beta std','O2 beta m','P8 delta std','P8 delta m','P8 theta std','P8 theta m','P8 alpha std','P8 alpha m','P8 beta std','P8 beta m','T8 delta std','T8 delta m','T8 theta std','T8 theta m','T8 alpha std','T8 alpha m','T8 beta std','T8 beta m','FC6 delta std','FC6 delta m','FC6 theta std','FC6 theta m','FC6 alpha std','FC6 alpha m','FC6 beta std','FC6 beta m','F4 delta std','F4 delta m','F4 theta std','F4 theta m','F4 alpha std','F4 alpha m','F4 beta std','F4 beta m','F8 delta std','F8 delta_m','F8 theta std','F8 theta m','F8 alpha std','F8 alpha m','F8 beta std','F8 beta m','AF4 delta std','AF4 delta m','AF4 theta std','AF4 theta m','AF4 alpha std','AF4 alpha m','AF4 beta std','AF4 beta m']\n",
    "column_names=['AF3 delta std','AF3 delta m','AF3 theta std','AF3 theta m','AF3 alpha std','AF3 alpha m','AF3 beta std','AF3 beta m','F7 delta std','F7 delta m','F7 theta std','F7 theta m','F7 alpha std','F7 alpha m','F7 beta std','F7 beta m','F3 delta std','F3 delta m','F3 theta std','F3 theta m','F3 alpha std','F3 alpha m','F3 beta std','F3 beta m','FC5 delta std','FC5 delta m','FC5 theta std','FC5 theta m','FC5 alpha std','FC5 alpha m','FC5 beta std','FC5 beta m','T7 delta std','T7 delta m','T7 theta std','T7 theta m','T7 alpha std','T7 alpha m','T7 beta std','T7 beta m','P7 delta std','P7 delta m','P7 theta std','P7 theta m','P7 alpha std','P7 alpha m','P7 beta std','P7 beta m','O1 delta std','O1 delta m','O1 theta std','O1 theta m','O1 alpha std','O1 alpha m','O1 beta std','O1 beta m','O2 delta std','O2 delta m','O2 theta std','O2 theta m','O2 alpha std','O2 alpha m','O2 beta std','O2 beta m','P8 delta std','P8 delta m','P8 theta std','P8 theta m','P8 alpha std','P8 alpha m','P8 beta std','P8 beta m','T8 delta std','T8 delta m','T8 theta std','T8 theta m','T8 alpha std','T8 alpha m','T8 beta std','T8 beta m','FC6 delta std','FC6 delta m','FC6 theta std','FC6 theta m','FC6 alpha std','FC6 alpha m','FC6 beta std','FC6 beta m','F4 delta std','F4 delta m','F4 theta std','F4 theta m','F4 alpha std','F4 alpha m','F4 beta std','F4 beta m','F8 delta std','F8 delta_m','F8 theta std','F8 theta m','F8 alpha std','F8 alpha m','F8 beta std','F8 beta m','AF4 delta std','AF4 delta m','AF4 theta std','AF4 theta m','AF4 alpha std','AF4 alpha m','AF4 beta std','AF4 beta m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>AF3 delta std</th>\n",
       "      <th>AF3 delta m</th>\n",
       "      <th>AF3 theta std</th>\n",
       "      <th>AF3 theta m</th>\n",
       "      <th>AF3 alpha std</th>\n",
       "      <th>AF3 alpha m</th>\n",
       "      <th>AF3 beta std</th>\n",
       "      <th>AF3 beta m</th>\n",
       "      <th>F7 delta std</th>\n",
       "      <th>...</th>\n",
       "      <th>F8 beta std</th>\n",
       "      <th>F8 beta m</th>\n",
       "      <th>AF4 delta std</th>\n",
       "      <th>AF4 delta m</th>\n",
       "      <th>AF4 theta std</th>\n",
       "      <th>AF4 theta m</th>\n",
       "      <th>AF4 alpha std</th>\n",
       "      <th>AF4 alpha m</th>\n",
       "      <th>AF4 beta std</th>\n",
       "      <th>AF4 beta m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.009311</td>\n",
       "      <td>0.009313</td>\n",
       "      <td>0.012881</td>\n",
       "      <td>0.015021</td>\n",
       "      <td>0.004803</td>\n",
       "      <td>0.011879</td>\n",
       "      <td>0.005595</td>\n",
       "      <td>0.009634</td>\n",
       "      <td>0.009305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024823</td>\n",
       "      <td>0.036521</td>\n",
       "      <td>0.009635</td>\n",
       "      <td>0.009790</td>\n",
       "      <td>0.038013</td>\n",
       "      <td>0.033612</td>\n",
       "      <td>0.031620</td>\n",
       "      <td>0.034734</td>\n",
       "      <td>0.021723</td>\n",
       "      <td>0.032796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.009309</td>\n",
       "      <td>0.009309</td>\n",
       "      <td>0.014605</td>\n",
       "      <td>0.012602</td>\n",
       "      <td>0.011903</td>\n",
       "      <td>0.007337</td>\n",
       "      <td>0.005365</td>\n",
       "      <td>0.007428</td>\n",
       "      <td>0.009304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019955</td>\n",
       "      <td>0.033712</td>\n",
       "      <td>0.009698</td>\n",
       "      <td>0.009779</td>\n",
       "      <td>0.018431</td>\n",
       "      <td>0.030900</td>\n",
       "      <td>0.038725</td>\n",
       "      <td>0.024312</td>\n",
       "      <td>0.018099</td>\n",
       "      <td>0.030079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.009308</td>\n",
       "      <td>0.009306</td>\n",
       "      <td>0.021534</td>\n",
       "      <td>0.008701</td>\n",
       "      <td>0.010227</td>\n",
       "      <td>0.007751</td>\n",
       "      <td>0.003857</td>\n",
       "      <td>0.006588</td>\n",
       "      <td>0.009303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022249</td>\n",
       "      <td>0.033654</td>\n",
       "      <td>0.009696</td>\n",
       "      <td>0.009764</td>\n",
       "      <td>0.037944</td>\n",
       "      <td>0.019194</td>\n",
       "      <td>0.028406</td>\n",
       "      <td>0.016154</td>\n",
       "      <td>0.020282</td>\n",
       "      <td>0.030218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.009307</td>\n",
       "      <td>0.009305</td>\n",
       "      <td>0.016791</td>\n",
       "      <td>0.012202</td>\n",
       "      <td>0.005746</td>\n",
       "      <td>0.008530</td>\n",
       "      <td>0.003362</td>\n",
       "      <td>0.006515</td>\n",
       "      <td>0.009302</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020787</td>\n",
       "      <td>0.032193</td>\n",
       "      <td>0.009699</td>\n",
       "      <td>0.009765</td>\n",
       "      <td>0.041242</td>\n",
       "      <td>0.023967</td>\n",
       "      <td>0.031201</td>\n",
       "      <td>0.021241</td>\n",
       "      <td>0.018423</td>\n",
       "      <td>0.028991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.009301</td>\n",
       "      <td>0.009309</td>\n",
       "      <td>0.012969</td>\n",
       "      <td>0.011437</td>\n",
       "      <td>0.006184</td>\n",
       "      <td>0.007106</td>\n",
       "      <td>0.003643</td>\n",
       "      <td>0.005571</td>\n",
       "      <td>0.009302</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019303</td>\n",
       "      <td>0.032504</td>\n",
       "      <td>0.009692</td>\n",
       "      <td>0.009723</td>\n",
       "      <td>0.017613</td>\n",
       "      <td>0.021440</td>\n",
       "      <td>0.029478</td>\n",
       "      <td>0.029471</td>\n",
       "      <td>0.015913</td>\n",
       "      <td>0.029850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 113 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class  AF3 delta std  AF3 delta m  AF3 theta std  AF3 theta m  \\\n",
       "0    1.0       0.009311     0.009313       0.012881     0.015021   \n",
       "1    1.0       0.009309     0.009309       0.014605     0.012602   \n",
       "2    1.0       0.009308     0.009306       0.021534     0.008701   \n",
       "3    1.0       0.009307     0.009305       0.016791     0.012202   \n",
       "4    1.0       0.009301     0.009309       0.012969     0.011437   \n",
       "\n",
       "   AF3 alpha std  AF3 alpha m  AF3 beta std  AF3 beta m  F7 delta std  ...  \\\n",
       "0       0.004803     0.011879      0.005595    0.009634      0.009305  ...   \n",
       "1       0.011903     0.007337      0.005365    0.007428      0.009304  ...   \n",
       "2       0.010227     0.007751      0.003857    0.006588      0.009303  ...   \n",
       "3       0.005746     0.008530      0.003362    0.006515      0.009302  ...   \n",
       "4       0.006184     0.007106      0.003643    0.005571      0.009302  ...   \n",
       "\n",
       "   F8 beta std  F8 beta m  AF4 delta std  AF4 delta m  AF4 theta std  \\\n",
       "0     0.024823   0.036521       0.009635     0.009790       0.038013   \n",
       "1     0.019955   0.033712       0.009698     0.009779       0.018431   \n",
       "2     0.022249   0.033654       0.009696     0.009764       0.037944   \n",
       "3     0.020787   0.032193       0.009699     0.009765       0.041242   \n",
       "4     0.019303   0.032504       0.009692     0.009723       0.017613   \n",
       "\n",
       "   AF4 theta m  AF4 alpha std  AF4 alpha m  AF4 beta std  AF4 beta m  \n",
       "0     0.033612       0.031620     0.034734      0.021723    0.032796  \n",
       "1     0.030900       0.038725     0.024312      0.018099    0.030079  \n",
       "2     0.019194       0.028406     0.016154      0.020282    0.030218  \n",
       "3     0.023967       0.031201     0.021241      0.018423    0.028991  \n",
       "4     0.021440       0.029478     0.029471      0.015913    0.029850  \n",
       "\n",
       "[5 rows x 113 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data normalization\n",
    "\n",
    "EEG_Data[column_names] = preprocessing.normalize(EEG_Data[column_names], axis=0)\n",
    "df = pd.DataFrame(EEG_Data, columns=all_columns)\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('normalized_EEG') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating x and y training sets\n",
    "\n",
    "x = df[['AF3 delta std','AF3 delta m','AF3 theta std','AF3 theta m','AF3 alpha std','AF3 alpha m','AF3 beta std','AF3 beta m','F7 delta std','F7 delta m','F7 theta std','F7 theta m','F7 alpha std','F7 alpha m','F7 beta std','F7 beta m','F3 delta std','F3 delta m','F3 theta std','F3 theta m','F3 alpha std','F3 alpha m','F3 beta std','F3 beta m','FC5 delta std','FC5 delta m','FC5 theta std','FC5 theta m','FC5 alpha std','FC5 alpha m','FC5 beta std','FC5 beta m','T7 delta std','T7 delta m','T7 theta std','T7 theta m','T7 alpha std','T7 alpha m','T7 beta std','T7 beta m','P7 delta std','P7 delta m','P7 theta std','P7 theta m','P7 alpha std','P7 alpha m','P7 beta std','P7 beta m','O1 delta std','O1 delta m','O1 theta std','O1 theta m','O1 alpha std','O1 alpha m','O1 beta std','O1 beta m','O2 delta std','O2 delta m','O2 theta std','O2 theta m','O2 alpha std','O2 alpha m','O2 beta std','O2 beta m','P8 delta std','P8 delta m','P8 theta std','P8 theta m','P8 alpha std','P8 alpha m','P8 beta std','P8 beta m','T8 delta std','T8 delta m','T8 theta std','T8 theta m','T8 alpha std','T8 alpha m','T8 beta std','T8 beta m','FC6 delta std','FC6 delta m','FC6 theta std','FC6 theta m','FC6 alpha std','FC6 alpha m','FC6 beta std','FC6 beta m','F4 delta std','F4 delta m','F4 theta std','F4 theta m','F4 alpha std','F4 alpha m','F4 beta std','F4 beta m','F8 delta std','F8 delta_m','F8 theta std','F8 theta m','F8 alpha std','F8 alpha m','F8 beta std','F8 beta m','AF4 delta std','AF4 delta m','AF4 theta std','AF4 theta m','AF4 alpha std','AF4 alpha m','AF4 beta std','AF4 beta m']]\n",
    "y = df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating x and y training sets\n",
    "#WEIGHTED AVERAGE\n",
    "\n",
    "x_std = df_std[['AF3 delta std', 'AF3 theta std', 'AF3 alpha std',\n",
    "       'AF3 beta std', 'F7 delta std', 'F7 theta std', 'F7 alpha std',\n",
    "       'F7 beta std', 'F3 delta std', 'F3 theta std', 'F3 alpha std',\n",
    "       'F3 beta std', 'FC5 delta std', 'FC5 theta std', 'FC5 alpha std',\n",
    "       'FC5 beta std', 'T7 delta std', 'T7 theta std', 'T7 alpha std',\n",
    "       'T7 beta std', 'P7 delta std', 'P7 theta std', 'P7 alpha std',\n",
    "       'P7 beta std', 'O1 delta std', 'O1 theta std', 'O1 alpha std',\n",
    "       'O1 beta std', 'O2 delta std', 'O2 theta std', 'O2 alpha std',\n",
    "       'O2 beta std', 'P8 delta std', 'P8 theta std', 'P8 alpha std',\n",
    "       'P8 beta std', 'T8 delta std', 'T8 theta std', 'T8 alpha std',\n",
    "       'T8 beta std', 'FC6 delta std', 'FC6 theta std', 'FC6 alpha std',\n",
    "       'FC6 beta std', 'F4 delta std', 'F4 theta std', 'F4 alpha std',\n",
    "       'F4 beta std', 'F8 delta std', 'F8 theta std', 'F8 alpha std',\n",
    "       'F8 beta std', 'AF4 delta std', 'AF4 theta std', 'AF4 alpha std',\n",
    "       'AF4 beta std']]\n",
    "y_std = df_std['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating x and y training sets\n",
    "#Mean average\n",
    "\n",
    "x_m = df_m[['AF3 delta m', 'AF3 theta m', 'AF3 alpha m', 'AF3 beta m',\n",
    "       'F7 delta m', 'F7 theta m', 'F7 alpha m', 'F7 beta m', 'F3 delta m',\n",
    "       'F3 theta m', 'F3 alpha m', 'F3 beta m', 'FC5 delta m', 'FC5 theta m',\n",
    "       'FC5 alpha m', 'FC5 beta m', 'T7 delta m', 'T7 theta m', 'T7 alpha m',\n",
    "       'T7 beta m', 'P7 delta m', 'P7 theta m', 'P7 alpha m', 'P7 beta m',\n",
    "       'O1 delta m', 'O1 theta m', 'O1 alpha m', 'O1 beta m', 'O2 delta m',\n",
    "       'O2 theta m', 'O2 alpha m', 'O2 beta m', 'P8 delta m', 'P8 theta m',\n",
    "       'P8 alpha m', 'P8 beta m', 'T8 delta m', 'T8 theta m', 'T8 alpha m',\n",
    "       'T8 beta m', 'FC6 delta m', 'FC6 theta m', 'FC6 alpha m', 'FC6 beta m',\n",
    "       'F4 delta m', 'F4 theta m', 'F4 alpha m', 'F4 beta m', 'F8 delta_m',\n",
    "       'F8 theta m', 'F8 alpha m', 'F8 beta m', 'AF4 delta m', 'AF4 theta m',\n",
    "       'AF4 alpha m', 'AF4 beta m']]\n",
    "y_m = df_m['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)  #Train and test split\n",
    "\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x_m,y_m,test_size=0.3, random_state=42) \n",
    "\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x_std,y_std,test_size=0.3, random_state=42) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSVM=svm.SVC()\n",
    "modelSVM.fit(x_train,y_train)\n",
    "predictSVM = modelSVM.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inicializing weight and random seed\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_12 (Flatten)        (None, 112)               0         \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 300)               33900     \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65,010\n",
      "Trainable params: 65,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Creating layers\n",
    "\n",
    "modelANN = keras.models.Sequential()\n",
    "\n",
    "modelANN.add(keras.layers.Flatten(input_shape=[112,]))\n",
    "modelANN.add(keras.layers.Dense(300, activation=\"relu\"))\n",
    "modelANN.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "modelANN.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "modelANN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the model with desired loss function\n",
    "\n",
    "modelANN.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"sgd\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 1.0997 - accuracy: 0.3417 - val_loss: 1.0978 - val_accuracy: 0.3468\n",
      "Epoch 2/300\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 1.1003 - accuracy: 0.3316 - val_loss: 1.1003 - val_accuracy: 0.3694\n",
      "Epoch 3/300\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 1.1005 - accuracy: 0.3379 - val_loss: 1.0989 - val_accuracy: 0.3468\n",
      "Epoch 4/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.1000 - accuracy: 0.3408 - val_loss: 1.1027 - val_accuracy: 0.3390\n",
      "Epoch 5/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0998 - accuracy: 0.3414 - val_loss: 1.1016 - val_accuracy: 0.3207\n",
      "Epoch 6/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0994 - accuracy: 0.3400 - val_loss: 1.0991 - val_accuracy: 0.3468\n",
      "Epoch 7/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0995 - accuracy: 0.3446 - val_loss: 1.1021 - val_accuracy: 0.3142\n",
      "Epoch 8/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0995 - accuracy: 0.3380 - val_loss: 1.1066 - val_accuracy: 0.3142\n",
      "Epoch 9/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.1001 - accuracy: 0.3375 - val_loss: 1.0983 - val_accuracy: 0.3572\n",
      "Epoch 10/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0999 - accuracy: 0.3328 - val_loss: 1.1009 - val_accuracy: 0.3138\n",
      "Epoch 11/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0991 - accuracy: 0.3474 - val_loss: 1.0988 - val_accuracy: 0.3464\n",
      "Epoch 12/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.1000 - accuracy: 0.3321 - val_loss: 1.0983 - val_accuracy: 0.3468\n",
      "Epoch 13/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.1003 - accuracy: 0.3379 - val_loss: 1.0992 - val_accuracy: 0.3433\n",
      "Epoch 14/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.1002 - accuracy: 0.3356 - val_loss: 1.0987 - val_accuracy: 0.3377\n",
      "Epoch 15/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0995 - accuracy: 0.3385 - val_loss: 1.0984 - val_accuracy: 0.3455\n",
      "Epoch 16/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.1000 - accuracy: 0.3334 - val_loss: 1.0993 - val_accuracy: 0.3194\n",
      "Epoch 17/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0995 - accuracy: 0.3366 - val_loss: 1.1025 - val_accuracy: 0.3142\n",
      "Epoch 18/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.1002 - accuracy: 0.3271 - val_loss: 1.1000 - val_accuracy: 0.3199\n",
      "Epoch 19/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.1002 - accuracy: 0.3341 - val_loss: 1.0994 - val_accuracy: 0.3459\n",
      "Epoch 20/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0997 - accuracy: 0.3443 - val_loss: 1.0998 - val_accuracy: 0.3207\n",
      "Epoch 21/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0996 - accuracy: 0.3393 - val_loss: 1.0993 - val_accuracy: 0.3481\n",
      "Epoch 22/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0997 - accuracy: 0.3380 - val_loss: 1.1040 - val_accuracy: 0.3142\n",
      "Epoch 23/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0999 - accuracy: 0.3368 - val_loss: 1.0983 - val_accuracy: 0.3546\n",
      "Epoch 24/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0995 - accuracy: 0.3392 - val_loss: 1.1056 - val_accuracy: 0.3138\n",
      "Epoch 25/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0995 - accuracy: 0.3432 - val_loss: 1.1060 - val_accuracy: 0.3142\n",
      "Epoch 26/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0995 - accuracy: 0.3370 - val_loss: 1.0996 - val_accuracy: 0.3394\n",
      "Epoch 27/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0985 - accuracy: 0.3467 - val_loss: 1.1069 - val_accuracy: 0.3142\n",
      "Epoch 28/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.1000 - accuracy: 0.3400 - val_loss: 1.1009 - val_accuracy: 0.3155\n",
      "Epoch 29/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0996 - accuracy: 0.3369 - val_loss: 1.0973 - val_accuracy: 0.3542\n",
      "Epoch 30/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0995 - accuracy: 0.3401 - val_loss: 1.1016 - val_accuracy: 0.3142\n",
      "Epoch 31/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0996 - accuracy: 0.3373 - val_loss: 1.1014 - val_accuracy: 0.3194\n",
      "Epoch 32/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0992 - accuracy: 0.3396 - val_loss: 1.0975 - val_accuracy: 0.3533\n",
      "Epoch 33/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0994 - accuracy: 0.3392 - val_loss: 1.0981 - val_accuracy: 0.3468\n",
      "Epoch 34/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0996 - accuracy: 0.3394 - val_loss: 1.0991 - val_accuracy: 0.3442\n",
      "Epoch 35/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0992 - accuracy: 0.3396 - val_loss: 1.0993 - val_accuracy: 0.3468\n",
      "Epoch 36/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0993 - accuracy: 0.3435 - val_loss: 1.0985 - val_accuracy: 0.3468\n",
      "Epoch 37/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0996 - accuracy: 0.3379 - val_loss: 1.0978 - val_accuracy: 0.3477\n",
      "Epoch 38/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0995 - accuracy: 0.3331 - val_loss: 1.0971 - val_accuracy: 0.3472\n",
      "Epoch 39/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0993 - accuracy: 0.3377 - val_loss: 1.1021 - val_accuracy: 0.3394\n",
      "Epoch 40/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0990 - accuracy: 0.3408 - val_loss: 1.0987 - val_accuracy: 0.3468\n",
      "Epoch 41/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0995 - accuracy: 0.3401 - val_loss: 1.0999 - val_accuracy: 0.3468\n",
      "Epoch 42/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0993 - accuracy: 0.3413 - val_loss: 1.0977 - val_accuracy: 0.3477\n",
      "Epoch 43/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0994 - accuracy: 0.3396 - val_loss: 1.1027 - val_accuracy: 0.3155\n",
      "Epoch 44/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0996 - accuracy: 0.3342 - val_loss: 1.0979 - val_accuracy: 0.3468\n",
      "Epoch 45/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0990 - accuracy: 0.3421 - val_loss: 1.0989 - val_accuracy: 0.3520\n",
      "Epoch 46/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0990 - accuracy: 0.3404 - val_loss: 1.0975 - val_accuracy: 0.3468\n",
      "Epoch 47/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0992 - accuracy: 0.3334 - val_loss: 1.1079 - val_accuracy: 0.3142\n",
      "Epoch 48/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0993 - accuracy: 0.3356 - val_loss: 1.0978 - val_accuracy: 0.3477\n",
      "Epoch 49/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0996 - accuracy: 0.3330 - val_loss: 1.0972 - val_accuracy: 0.3550\n",
      "Epoch 50/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0988 - accuracy: 0.3433 - val_loss: 1.0998 - val_accuracy: 0.3468\n",
      "Epoch 51/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0993 - accuracy: 0.3396 - val_loss: 1.0969 - val_accuracy: 0.3520\n",
      "Epoch 52/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0994 - accuracy: 0.3403 - val_loss: 1.0974 - val_accuracy: 0.3468\n",
      "Epoch 53/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0995 - accuracy: 0.3403 - val_loss: 1.0998 - val_accuracy: 0.3312\n",
      "Epoch 54/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0991 - accuracy: 0.3401 - val_loss: 1.0989 - val_accuracy: 0.3468\n",
      "Epoch 55/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0996 - accuracy: 0.3350 - val_loss: 1.1011 - val_accuracy: 0.3164\n",
      "Epoch 56/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0991 - accuracy: 0.3410 - val_loss: 1.0987 - val_accuracy: 0.3477\n",
      "Epoch 57/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0991 - accuracy: 0.3402 - val_loss: 1.1034 - val_accuracy: 0.3407\n",
      "Epoch 58/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0993 - accuracy: 0.3411 - val_loss: 1.0998 - val_accuracy: 0.3207\n",
      "Epoch 59/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0990 - accuracy: 0.3439 - val_loss: 1.1040 - val_accuracy: 0.3155\n",
      "Epoch 60/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0995 - accuracy: 0.3384 - val_loss: 1.0979 - val_accuracy: 0.3507\n",
      "Epoch 61/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0990 - accuracy: 0.3402 - val_loss: 1.0996 - val_accuracy: 0.3212\n",
      "Epoch 62/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0990 - accuracy: 0.3418 - val_loss: 1.0983 - val_accuracy: 0.3468\n",
      "Epoch 63/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0995 - accuracy: 0.3319 - val_loss: 1.0989 - val_accuracy: 0.3477\n",
      "Epoch 64/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0989 - accuracy: 0.3436 - val_loss: 1.1004 - val_accuracy: 0.3199\n",
      "Epoch 65/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0993 - accuracy: 0.3413 - val_loss: 1.0982 - val_accuracy: 0.3490\n",
      "Epoch 66/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0986 - accuracy: 0.3433 - val_loss: 1.1052 - val_accuracy: 0.3138\n",
      "Epoch 67/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0990 - accuracy: 0.3438 - val_loss: 1.0974 - val_accuracy: 0.3715\n",
      "Epoch 68/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0986 - accuracy: 0.3475 - val_loss: 1.0980 - val_accuracy: 0.3542\n",
      "Epoch 69/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0989 - accuracy: 0.3398 - val_loss: 1.0996 - val_accuracy: 0.3637\n",
      "Epoch 70/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0992 - accuracy: 0.3420 - val_loss: 1.0988 - val_accuracy: 0.3468\n",
      "Epoch 71/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0989 - accuracy: 0.3398 - val_loss: 1.0971 - val_accuracy: 0.3542\n",
      "Epoch 72/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0994 - accuracy: 0.3376 - val_loss: 1.1006 - val_accuracy: 0.3190\n",
      "Epoch 73/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0989 - accuracy: 0.3415 - val_loss: 1.0971 - val_accuracy: 0.3472\n",
      "Epoch 74/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0988 - accuracy: 0.3465 - val_loss: 1.1013 - val_accuracy: 0.3194\n",
      "Epoch 75/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0993 - accuracy: 0.3404 - val_loss: 1.0986 - val_accuracy: 0.3468\n",
      "Epoch 76/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0986 - accuracy: 0.3429 - val_loss: 1.0990 - val_accuracy: 0.3312\n",
      "Epoch 77/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0989 - accuracy: 0.3441 - val_loss: 1.0965 - val_accuracy: 0.3555\n",
      "Epoch 78/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0985 - accuracy: 0.3468 - val_loss: 1.1050 - val_accuracy: 0.3394\n",
      "Epoch 79/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0989 - accuracy: 0.3403 - val_loss: 1.1034 - val_accuracy: 0.3207\n",
      "Epoch 80/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0991 - accuracy: 0.3365 - val_loss: 1.0980 - val_accuracy: 0.3472\n",
      "Epoch 81/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0990 - accuracy: 0.3445 - val_loss: 1.1018 - val_accuracy: 0.3181\n",
      "Epoch 82/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0993 - accuracy: 0.3327 - val_loss: 1.0971 - val_accuracy: 0.3511\n",
      "Epoch 83/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0989 - accuracy: 0.3427 - val_loss: 1.0970 - val_accuracy: 0.3459\n",
      "Epoch 84/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0983 - accuracy: 0.3488 - val_loss: 1.1033 - val_accuracy: 0.3164\n",
      "Epoch 85/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0988 - accuracy: 0.3436 - val_loss: 1.1003 - val_accuracy: 0.3451\n",
      "Epoch 86/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0987 - accuracy: 0.3422 - val_loss: 1.0997 - val_accuracy: 0.3220\n",
      "Epoch 87/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0986 - accuracy: 0.3487 - val_loss: 1.0969 - val_accuracy: 0.3468\n",
      "Epoch 88/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0989 - accuracy: 0.3398 - val_loss: 1.1008 - val_accuracy: 0.3234\n",
      "Epoch 89/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0989 - accuracy: 0.3369 - val_loss: 1.0979 - val_accuracy: 0.3594\n",
      "Epoch 90/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0986 - accuracy: 0.3385 - val_loss: 1.1025 - val_accuracy: 0.3398\n",
      "Epoch 91/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0983 - accuracy: 0.3475 - val_loss: 1.0964 - val_accuracy: 0.3485\n",
      "Epoch 92/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0988 - accuracy: 0.3426 - val_loss: 1.1035 - val_accuracy: 0.3190\n",
      "Epoch 93/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0990 - accuracy: 0.3439 - val_loss: 1.1044 - val_accuracy: 0.3164\n",
      "Epoch 94/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0986 - accuracy: 0.3409 - val_loss: 1.1013 - val_accuracy: 0.3372\n",
      "Epoch 95/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0985 - accuracy: 0.3481 - val_loss: 1.0985 - val_accuracy: 0.3672\n",
      "Epoch 96/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0988 - accuracy: 0.3421 - val_loss: 1.0971 - val_accuracy: 0.3537\n",
      "Epoch 97/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0984 - accuracy: 0.3393 - val_loss: 1.0972 - val_accuracy: 0.3485\n",
      "Epoch 98/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0986 - accuracy: 0.3403 - val_loss: 1.1006 - val_accuracy: 0.3194\n",
      "Epoch 99/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0987 - accuracy: 0.3435 - val_loss: 1.0996 - val_accuracy: 0.3459\n",
      "Epoch 100/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0987 - accuracy: 0.3493 - val_loss: 1.0994 - val_accuracy: 0.3234\n",
      "Epoch 101/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0984 - accuracy: 0.3422 - val_loss: 1.0968 - val_accuracy: 0.3490\n",
      "Epoch 102/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0979 - accuracy: 0.3458 - val_loss: 1.1054 - val_accuracy: 0.3190\n",
      "Epoch 103/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0984 - accuracy: 0.3417 - val_loss: 1.0978 - val_accuracy: 0.3511\n",
      "Epoch 104/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0987 - accuracy: 0.3471 - val_loss: 1.0967 - val_accuracy: 0.3581\n",
      "Epoch 105/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0983 - accuracy: 0.3502 - val_loss: 1.0993 - val_accuracy: 0.3238\n",
      "Epoch 106/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0987 - accuracy: 0.3438 - val_loss: 1.0973 - val_accuracy: 0.3550\n",
      "Epoch 107/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0986 - accuracy: 0.3431 - val_loss: 1.1017 - val_accuracy: 0.3242\n",
      "Epoch 108/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0983 - accuracy: 0.3446 - val_loss: 1.0971 - val_accuracy: 0.3446\n",
      "Epoch 109/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0987 - accuracy: 0.3440 - val_loss: 1.1006 - val_accuracy: 0.3238\n",
      "Epoch 110/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0982 - accuracy: 0.3523 - val_loss: 1.0983 - val_accuracy: 0.3416\n",
      "Epoch 111/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0988 - accuracy: 0.3454 - val_loss: 1.0986 - val_accuracy: 0.3320\n",
      "Epoch 112/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0986 - accuracy: 0.3474 - val_loss: 1.0967 - val_accuracy: 0.3546\n",
      "Epoch 113/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0988 - accuracy: 0.3448 - val_loss: 1.0993 - val_accuracy: 0.3260\n",
      "Epoch 114/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0984 - accuracy: 0.3408 - val_loss: 1.0995 - val_accuracy: 0.3472\n",
      "Epoch 115/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0986 - accuracy: 0.3430 - val_loss: 1.0965 - val_accuracy: 0.3507\n",
      "Epoch 116/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0974 - accuracy: 0.3573 - val_loss: 1.1007 - val_accuracy: 0.3429\n",
      "Epoch 117/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0980 - accuracy: 0.3473 - val_loss: 1.0983 - val_accuracy: 0.3351\n",
      "Epoch 118/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0983 - accuracy: 0.3468 - val_loss: 1.0976 - val_accuracy: 0.3442\n",
      "Epoch 119/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0983 - accuracy: 0.3461 - val_loss: 1.0970 - val_accuracy: 0.3446\n",
      "Epoch 120/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0984 - accuracy: 0.3502 - val_loss: 1.0979 - val_accuracy: 0.3433\n",
      "Epoch 121/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0978 - accuracy: 0.3486 - val_loss: 1.0985 - val_accuracy: 0.3346\n",
      "Epoch 122/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0984 - accuracy: 0.3490 - val_loss: 1.0974 - val_accuracy: 0.3416\n",
      "Epoch 123/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0980 - accuracy: 0.3449 - val_loss: 1.0960 - val_accuracy: 0.3516\n",
      "Epoch 124/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0986 - accuracy: 0.3433 - val_loss: 1.0982 - val_accuracy: 0.3537\n",
      "Epoch 125/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0976 - accuracy: 0.3531 - val_loss: 1.0984 - val_accuracy: 0.3446\n",
      "Epoch 126/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0980 - accuracy: 0.3492 - val_loss: 1.1021 - val_accuracy: 0.3407\n",
      "Epoch 127/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0981 - accuracy: 0.3468 - val_loss: 1.0984 - val_accuracy: 0.3424\n",
      "Epoch 128/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0979 - accuracy: 0.3506 - val_loss: 1.1013 - val_accuracy: 0.3359\n",
      "Epoch 129/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0979 - accuracy: 0.3469 - val_loss: 1.0970 - val_accuracy: 0.3589\n",
      "Epoch 130/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0981 - accuracy: 0.3457 - val_loss: 1.1007 - val_accuracy: 0.3290\n",
      "Epoch 131/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0980 - accuracy: 0.3455 - val_loss: 1.0957 - val_accuracy: 0.3477\n",
      "Epoch 132/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0981 - accuracy: 0.3483 - val_loss: 1.1016 - val_accuracy: 0.3194\n",
      "Epoch 133/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0980 - accuracy: 0.3421 - val_loss: 1.0974 - val_accuracy: 0.3520\n",
      "Epoch 134/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0973 - accuracy: 0.3529 - val_loss: 1.1021 - val_accuracy: 0.3212\n",
      "Epoch 135/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0982 - accuracy: 0.3453 - val_loss: 1.0957 - val_accuracy: 0.3516\n",
      "Epoch 136/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0981 - accuracy: 0.3438 - val_loss: 1.0967 - val_accuracy: 0.3555\n",
      "Epoch 137/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0981 - accuracy: 0.3500 - val_loss: 1.1009 - val_accuracy: 0.3212\n",
      "Epoch 138/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0983 - accuracy: 0.3480 - val_loss: 1.0968 - val_accuracy: 0.3550\n",
      "Epoch 139/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0979 - accuracy: 0.3494 - val_loss: 1.0956 - val_accuracy: 0.3520\n",
      "Epoch 140/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0982 - accuracy: 0.3443 - val_loss: 1.0963 - val_accuracy: 0.3546\n",
      "Epoch 141/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0980 - accuracy: 0.3496 - val_loss: 1.0957 - val_accuracy: 0.3511\n",
      "Epoch 142/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0981 - accuracy: 0.3418 - val_loss: 1.0973 - val_accuracy: 0.3451\n",
      "Epoch 143/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0977 - accuracy: 0.3523 - val_loss: 1.0967 - val_accuracy: 0.3542\n",
      "Epoch 144/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0977 - accuracy: 0.3480 - val_loss: 1.0968 - val_accuracy: 0.3420\n",
      "Epoch 145/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0977 - accuracy: 0.3448 - val_loss: 1.0966 - val_accuracy: 0.3542\n",
      "Epoch 146/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0981 - accuracy: 0.3462 - val_loss: 1.0987 - val_accuracy: 0.3503\n",
      "Epoch 147/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0975 - accuracy: 0.3473 - val_loss: 1.1006 - val_accuracy: 0.3364\n",
      "Epoch 148/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0976 - accuracy: 0.3493 - val_loss: 1.0974 - val_accuracy: 0.3468\n",
      "Epoch 149/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0976 - accuracy: 0.3512 - val_loss: 1.0994 - val_accuracy: 0.3294\n",
      "Epoch 150/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0976 - accuracy: 0.3546 - val_loss: 1.0976 - val_accuracy: 0.3607\n",
      "Epoch 151/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0978 - accuracy: 0.3433 - val_loss: 1.0961 - val_accuracy: 0.3546\n",
      "Epoch 152/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0979 - accuracy: 0.3444 - val_loss: 1.0986 - val_accuracy: 0.3468\n",
      "Epoch 153/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0976 - accuracy: 0.3491 - val_loss: 1.0986 - val_accuracy: 0.3442\n",
      "Epoch 154/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0977 - accuracy: 0.3486 - val_loss: 1.0980 - val_accuracy: 0.3520\n",
      "Epoch 155/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0975 - accuracy: 0.3520 - val_loss: 1.0992 - val_accuracy: 0.3333\n",
      "Epoch 156/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0978 - accuracy: 0.3459 - val_loss: 1.0966 - val_accuracy: 0.3420\n",
      "Epoch 157/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0975 - accuracy: 0.3439 - val_loss: 1.0984 - val_accuracy: 0.3355\n",
      "Epoch 158/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0976 - accuracy: 0.3479 - val_loss: 1.0963 - val_accuracy: 0.3694\n",
      "Epoch 159/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0977 - accuracy: 0.3498 - val_loss: 1.0968 - val_accuracy: 0.3446\n",
      "Epoch 160/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0977 - accuracy: 0.3439 - val_loss: 1.0970 - val_accuracy: 0.3451\n",
      "Epoch 161/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0973 - accuracy: 0.3502 - val_loss: 1.0970 - val_accuracy: 0.3455\n",
      "Epoch 162/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0974 - accuracy: 0.3584 - val_loss: 1.1006 - val_accuracy: 0.3290\n",
      "Epoch 163/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0975 - accuracy: 0.3542 - val_loss: 1.0961 - val_accuracy: 0.3676\n",
      "Epoch 164/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0972 - accuracy: 0.3535 - val_loss: 1.0981 - val_accuracy: 0.3529\n",
      "Epoch 165/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0972 - accuracy: 0.3502 - val_loss: 1.1005 - val_accuracy: 0.3429\n",
      "Epoch 166/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0971 - accuracy: 0.3482 - val_loss: 1.0957 - val_accuracy: 0.3542\n",
      "Epoch 167/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0976 - accuracy: 0.3497 - val_loss: 1.0976 - val_accuracy: 0.3438\n",
      "Epoch 168/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0977 - accuracy: 0.3444 - val_loss: 1.0957 - val_accuracy: 0.3854\n",
      "Epoch 169/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0976 - accuracy: 0.3493 - val_loss: 1.0970 - val_accuracy: 0.3555\n",
      "Epoch 170/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0974 - accuracy: 0.3520 - val_loss: 1.1023 - val_accuracy: 0.3229\n",
      "Epoch 171/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0972 - accuracy: 0.3551 - val_loss: 1.0967 - val_accuracy: 0.3559\n",
      "Epoch 172/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0975 - accuracy: 0.3506 - val_loss: 1.0959 - val_accuracy: 0.3472\n",
      "Epoch 173/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0972 - accuracy: 0.3521 - val_loss: 1.0960 - val_accuracy: 0.3624\n",
      "Epoch 174/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0971 - accuracy: 0.3502 - val_loss: 1.0964 - val_accuracy: 0.3555\n",
      "Epoch 175/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0970 - accuracy: 0.3568 - val_loss: 1.0964 - val_accuracy: 0.3563\n",
      "Epoch 176/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0970 - accuracy: 0.3532 - val_loss: 1.0972 - val_accuracy: 0.3424\n",
      "Epoch 177/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0973 - accuracy: 0.3507 - val_loss: 1.0949 - val_accuracy: 0.3468\n",
      "Epoch 178/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0972 - accuracy: 0.3441 - val_loss: 1.1082 - val_accuracy: 0.3207\n",
      "Epoch 179/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0976 - accuracy: 0.3455 - val_loss: 1.0956 - val_accuracy: 0.3546\n",
      "Epoch 180/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0973 - accuracy: 0.3522 - val_loss: 1.0985 - val_accuracy: 0.3355\n",
      "Epoch 181/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0975 - accuracy: 0.3465 - val_loss: 1.0991 - val_accuracy: 0.3312\n",
      "Epoch 182/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0969 - accuracy: 0.3525 - val_loss: 1.1010 - val_accuracy: 0.3234\n",
      "Epoch 183/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0970 - accuracy: 0.3504 - val_loss: 1.1036 - val_accuracy: 0.3212\n",
      "Epoch 184/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0971 - accuracy: 0.3542 - val_loss: 1.0963 - val_accuracy: 0.3550\n",
      "Epoch 185/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0973 - accuracy: 0.3485 - val_loss: 1.1001 - val_accuracy: 0.3281\n",
      "Epoch 186/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0966 - accuracy: 0.3534 - val_loss: 1.1011 - val_accuracy: 0.3234\n",
      "Epoch 187/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0972 - accuracy: 0.3560 - val_loss: 1.0979 - val_accuracy: 0.3420\n",
      "Epoch 188/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0969 - accuracy: 0.3528 - val_loss: 1.0951 - val_accuracy: 0.3481\n",
      "Epoch 189/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0965 - accuracy: 0.3610 - val_loss: 1.0953 - val_accuracy: 0.3468\n",
      "Epoch 190/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0973 - accuracy: 0.3563 - val_loss: 1.0959 - val_accuracy: 0.3542\n",
      "Epoch 191/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0968 - accuracy: 0.3503 - val_loss: 1.0949 - val_accuracy: 0.3546\n",
      "Epoch 192/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0968 - accuracy: 0.3559 - val_loss: 1.0996 - val_accuracy: 0.3429\n",
      "Epoch 193/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0974 - accuracy: 0.3529 - val_loss: 1.0952 - val_accuracy: 0.3772\n",
      "Epoch 194/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0970 - accuracy: 0.3480 - val_loss: 1.1002 - val_accuracy: 0.3498\n",
      "Epoch 195/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0963 - accuracy: 0.3636 - val_loss: 1.0951 - val_accuracy: 0.3663\n",
      "Epoch 196/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0968 - accuracy: 0.3481 - val_loss: 1.1001 - val_accuracy: 0.3286\n",
      "Epoch 197/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0967 - accuracy: 0.3569 - val_loss: 1.0984 - val_accuracy: 0.3338\n",
      "Epoch 198/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0970 - accuracy: 0.3524 - val_loss: 1.0953 - val_accuracy: 0.3537\n",
      "Epoch 199/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0974 - accuracy: 0.3486 - val_loss: 1.0966 - val_accuracy: 0.3446\n",
      "Epoch 200/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0968 - accuracy: 0.3548 - val_loss: 1.0962 - val_accuracy: 0.3472\n",
      "Epoch 201/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0973 - accuracy: 0.3484 - val_loss: 1.0976 - val_accuracy: 0.3451\n",
      "Epoch 202/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0968 - accuracy: 0.3566 - val_loss: 1.0957 - val_accuracy: 0.3594\n",
      "Epoch 203/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0971 - accuracy: 0.3528 - val_loss: 1.0956 - val_accuracy: 0.3563\n",
      "Epoch 204/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0965 - accuracy: 0.3493 - val_loss: 1.0960 - val_accuracy: 0.3472\n",
      "Epoch 205/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0971 - accuracy: 0.3491 - val_loss: 1.0962 - val_accuracy: 0.3446\n",
      "Epoch 206/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0967 - accuracy: 0.3507 - val_loss: 1.0960 - val_accuracy: 0.3420\n",
      "Epoch 207/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0967 - accuracy: 0.3569 - val_loss: 1.0952 - val_accuracy: 0.3490\n",
      "Epoch 208/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0967 - accuracy: 0.3549 - val_loss: 1.1040 - val_accuracy: 0.3333\n",
      "Epoch 209/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0970 - accuracy: 0.3528 - val_loss: 1.0943 - val_accuracy: 0.3494\n",
      "Epoch 210/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0966 - accuracy: 0.3541 - val_loss: 1.0959 - val_accuracy: 0.3511\n",
      "Epoch 211/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0969 - accuracy: 0.3486 - val_loss: 1.0945 - val_accuracy: 0.3503\n",
      "Epoch 212/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0961 - accuracy: 0.3609 - val_loss: 1.0963 - val_accuracy: 0.3438\n",
      "Epoch 213/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0966 - accuracy: 0.3500 - val_loss: 1.0952 - val_accuracy: 0.3477\n",
      "Epoch 214/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0965 - accuracy: 0.3518 - val_loss: 1.0961 - val_accuracy: 0.3433\n",
      "Epoch 215/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0959 - accuracy: 0.3553 - val_loss: 1.0948 - val_accuracy: 0.3646\n",
      "Epoch 216/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0964 - accuracy: 0.3589 - val_loss: 1.1028 - val_accuracy: 0.3468\n",
      "Epoch 217/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0965 - accuracy: 0.3580 - val_loss: 1.0948 - val_accuracy: 0.3490\n",
      "Epoch 218/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0966 - accuracy: 0.3538 - val_loss: 1.0967 - val_accuracy: 0.3451\n",
      "Epoch 219/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0967 - accuracy: 0.3517 - val_loss: 1.0968 - val_accuracy: 0.3555\n",
      "Epoch 220/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0964 - accuracy: 0.3553 - val_loss: 1.0959 - val_accuracy: 0.3555\n",
      "Epoch 221/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0960 - accuracy: 0.3577 - val_loss: 1.0961 - val_accuracy: 0.3420\n",
      "Epoch 222/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0964 - accuracy: 0.3520 - val_loss: 1.0968 - val_accuracy: 0.3420\n",
      "Epoch 223/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0959 - accuracy: 0.3528 - val_loss: 1.1027 - val_accuracy: 0.3229\n",
      "Epoch 224/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0961 - accuracy: 0.3557 - val_loss: 1.0952 - val_accuracy: 0.3533\n",
      "Epoch 225/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0962 - accuracy: 0.3604 - val_loss: 1.0983 - val_accuracy: 0.3411\n",
      "Epoch 226/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0964 - accuracy: 0.3539 - val_loss: 1.0994 - val_accuracy: 0.3511\n",
      "Epoch 227/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0965 - accuracy: 0.3522 - val_loss: 1.0943 - val_accuracy: 0.3511\n",
      "Epoch 228/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0962 - accuracy: 0.3554 - val_loss: 1.0969 - val_accuracy: 0.3411\n",
      "Epoch 229/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0963 - accuracy: 0.3589 - val_loss: 1.0952 - val_accuracy: 0.3477\n",
      "Epoch 230/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0964 - accuracy: 0.3543 - val_loss: 1.0996 - val_accuracy: 0.3312\n",
      "Epoch 231/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0961 - accuracy: 0.3523 - val_loss: 1.0975 - val_accuracy: 0.3420\n",
      "Epoch 232/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0965 - accuracy: 0.3573 - val_loss: 1.0957 - val_accuracy: 0.3442\n",
      "Epoch 233/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0965 - accuracy: 0.3524 - val_loss: 1.0958 - val_accuracy: 0.3468\n",
      "Epoch 234/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0963 - accuracy: 0.3491 - val_loss: 1.1013 - val_accuracy: 0.3398\n",
      "Epoch 235/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0961 - accuracy: 0.3535 - val_loss: 1.0947 - val_accuracy: 0.3537\n",
      "Epoch 236/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0954 - accuracy: 0.3574 - val_loss: 1.0959 - val_accuracy: 0.3442\n",
      "Epoch 237/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0963 - accuracy: 0.3509 - val_loss: 1.0956 - val_accuracy: 0.3468\n",
      "Epoch 238/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0960 - accuracy: 0.3549 - val_loss: 1.0963 - val_accuracy: 0.3542\n",
      "Epoch 239/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0958 - accuracy: 0.3547 - val_loss: 1.0973 - val_accuracy: 0.3537\n",
      "Epoch 240/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0960 - accuracy: 0.3586 - val_loss: 1.0950 - val_accuracy: 0.3546\n",
      "Epoch 241/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0958 - accuracy: 0.3532 - val_loss: 1.0937 - val_accuracy: 0.3529\n",
      "Epoch 242/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0961 - accuracy: 0.3541 - val_loss: 1.0941 - val_accuracy: 0.3503\n",
      "Epoch 243/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0963 - accuracy: 0.3586 - val_loss: 1.0946 - val_accuracy: 0.3694\n",
      "Epoch 244/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0954 - accuracy: 0.3604 - val_loss: 1.1000 - val_accuracy: 0.3424\n",
      "Epoch 245/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0959 - accuracy: 0.3600 - val_loss: 1.0941 - val_accuracy: 0.3524\n",
      "Epoch 246/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0958 - accuracy: 0.3521 - val_loss: 1.0950 - val_accuracy: 0.3537\n",
      "Epoch 247/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0961 - accuracy: 0.3529 - val_loss: 1.0939 - val_accuracy: 0.3533\n",
      "Epoch 248/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0961 - accuracy: 0.3586 - val_loss: 1.0950 - val_accuracy: 0.3472\n",
      "Epoch 249/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0960 - accuracy: 0.3544 - val_loss: 1.0956 - val_accuracy: 0.3529\n",
      "Epoch 250/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0959 - accuracy: 0.3582 - val_loss: 1.0937 - val_accuracy: 0.3546\n",
      "Epoch 251/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0960 - accuracy: 0.3562 - val_loss: 1.0962 - val_accuracy: 0.3624\n",
      "Epoch 252/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0959 - accuracy: 0.3605 - val_loss: 1.0936 - val_accuracy: 0.3633\n",
      "Epoch 253/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0964 - accuracy: 0.3569 - val_loss: 1.0944 - val_accuracy: 0.3550\n",
      "Epoch 254/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0961 - accuracy: 0.3523 - val_loss: 1.0935 - val_accuracy: 0.3772\n",
      "Epoch 255/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0958 - accuracy: 0.3535 - val_loss: 1.0972 - val_accuracy: 0.3416\n",
      "Epoch 256/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0953 - accuracy: 0.3640 - val_loss: 1.0976 - val_accuracy: 0.3420\n",
      "Epoch 257/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0958 - accuracy: 0.3589 - val_loss: 1.0951 - val_accuracy: 0.3438\n",
      "Epoch 258/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0955 - accuracy: 0.3610 - val_loss: 1.0991 - val_accuracy: 0.3420\n",
      "Epoch 259/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0950 - accuracy: 0.3585 - val_loss: 1.0937 - val_accuracy: 0.3550\n",
      "Epoch 260/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0959 - accuracy: 0.3598 - val_loss: 1.0944 - val_accuracy: 0.3459\n",
      "Epoch 261/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0954 - accuracy: 0.3553 - val_loss: 1.0935 - val_accuracy: 0.3776\n",
      "Epoch 262/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0954 - accuracy: 0.3550 - val_loss: 1.0942 - val_accuracy: 0.3668\n",
      "Epoch 263/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0950 - accuracy: 0.3576 - val_loss: 1.0946 - val_accuracy: 0.3672\n",
      "Epoch 264/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0955 - accuracy: 0.3517 - val_loss: 1.0959 - val_accuracy: 0.3550\n",
      "Epoch 265/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0952 - accuracy: 0.3604 - val_loss: 1.0973 - val_accuracy: 0.3411\n",
      "Epoch 266/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0953 - accuracy: 0.3630 - val_loss: 1.0942 - val_accuracy: 0.3537\n",
      "Epoch 267/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0951 - accuracy: 0.3592 - val_loss: 1.0946 - val_accuracy: 0.3464\n",
      "Epoch 268/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0960 - accuracy: 0.3532 - val_loss: 1.0954 - val_accuracy: 0.3533\n",
      "Epoch 269/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0952 - accuracy: 0.3640 - val_loss: 1.0934 - val_accuracy: 0.3806\n",
      "Epoch 270/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0954 - accuracy: 0.3515 - val_loss: 1.0933 - val_accuracy: 0.3646\n",
      "Epoch 271/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0956 - accuracy: 0.3526 - val_loss: 1.0941 - val_accuracy: 0.3529\n",
      "Epoch 272/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0949 - accuracy: 0.3564 - val_loss: 1.0929 - val_accuracy: 0.3533\n",
      "Epoch 273/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0953 - accuracy: 0.3563 - val_loss: 1.0959 - val_accuracy: 0.3429\n",
      "Epoch 274/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0955 - accuracy: 0.3574 - val_loss: 1.0936 - val_accuracy: 0.3663\n",
      "Epoch 275/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0955 - accuracy: 0.3569 - val_loss: 1.0934 - val_accuracy: 0.3581\n",
      "Epoch 276/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0949 - accuracy: 0.3614 - val_loss: 1.0972 - val_accuracy: 0.3407\n",
      "Epoch 277/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0953 - accuracy: 0.3602 - val_loss: 1.0968 - val_accuracy: 0.3420\n",
      "Epoch 278/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0952 - accuracy: 0.3563 - val_loss: 1.0944 - val_accuracy: 0.3507\n",
      "Epoch 279/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0949 - accuracy: 0.3662 - val_loss: 1.0972 - val_accuracy: 0.3438\n",
      "Epoch 280/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0951 - accuracy: 0.3538 - val_loss: 1.0944 - val_accuracy: 0.3498\n",
      "Epoch 281/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0950 - accuracy: 0.3619 - val_loss: 1.0936 - val_accuracy: 0.3546\n",
      "Epoch 282/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0958 - accuracy: 0.3557 - val_loss: 1.0936 - val_accuracy: 0.3759\n",
      "Epoch 283/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0948 - accuracy: 0.3634 - val_loss: 1.0983 - val_accuracy: 0.3503\n",
      "Epoch 284/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0955 - accuracy: 0.3530 - val_loss: 1.0933 - val_accuracy: 0.3589\n",
      "Epoch 285/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0954 - accuracy: 0.3570 - val_loss: 1.0955 - val_accuracy: 0.3438\n",
      "Epoch 286/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0952 - accuracy: 0.3547 - val_loss: 1.0936 - val_accuracy: 0.3737\n",
      "Epoch 287/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0951 - accuracy: 0.3562 - val_loss: 1.0955 - val_accuracy: 0.3555\n",
      "Epoch 288/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0953 - accuracy: 0.3574 - val_loss: 1.0972 - val_accuracy: 0.3338\n",
      "Epoch 289/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0954 - accuracy: 0.3586 - val_loss: 1.0939 - val_accuracy: 0.3533\n",
      "Epoch 290/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0948 - accuracy: 0.3652 - val_loss: 1.0947 - val_accuracy: 0.3477\n",
      "Epoch 291/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0951 - accuracy: 0.3618 - val_loss: 1.0926 - val_accuracy: 0.3702\n",
      "Epoch 292/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0940 - accuracy: 0.3626 - val_loss: 1.0970 - val_accuracy: 0.3403\n",
      "Epoch 293/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0945 - accuracy: 0.3576 - val_loss: 1.0947 - val_accuracy: 0.3442\n",
      "Epoch 294/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0954 - accuracy: 0.3576 - val_loss: 1.0931 - val_accuracy: 0.3598\n",
      "Epoch 295/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0949 - accuracy: 0.3568 - val_loss: 1.0961 - val_accuracy: 0.3563\n",
      "Epoch 296/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0946 - accuracy: 0.3621 - val_loss: 1.0926 - val_accuracy: 0.3568\n",
      "Epoch 297/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0946 - accuracy: 0.3572 - val_loss: 1.0982 - val_accuracy: 0.3364\n",
      "Epoch 298/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0951 - accuracy: 0.3586 - val_loss: 1.0944 - val_accuracy: 0.3546\n",
      "Epoch 299/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0949 - accuracy: 0.3604 - val_loss: 1.0924 - val_accuracy: 0.3585\n",
      "Epoch 300/300\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 1.0947 - accuracy: 0.3610 - val_loss: 1.0929 - val_accuracy: 0.3529\n"
     ]
    }
   ],
   "source": [
    "#Training the model\n",
    "\n",
    "model_history = modelANN.fit(x_train,y_train,epochs=300,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.04340277777778"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelSVM.score(x_test,y_test)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 0s 929us/step - loss: 1.0929 - accuracy: 0.3529\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408,\n",
       " 1.092939853668213,\n",
       " 0.3528645932674408]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelANN.evaluate(x_test,y_test)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      0.50      0.49       724\n",
      "         1.0       0.51      0.55      0.53       799\n",
      "         2.0       0.51      0.45      0.48       781\n",
      "\n",
      "    accuracy                           0.50      2304\n",
      "   macro avg       0.50      0.50      0.50      2304\n",
      "weighted avg       0.50      0.50      0.50      2304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictSVM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion matrix\n",
      " Predito  0.0  1.0  2.0  Todos\n",
      "Real                         \n",
      "0.0      363  201  160    724\n",
      "1.0      183  442  174    799\n",
      "2.0      210  223  348    781\n",
      "Todos    756  866  682   2304\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMatriz de Confusão\\r\\n\",\n",
    "pd.crosstab(y_test,predictSVM,rownames=['Real'],colnames=['Predito'], margins=True, margins_name='Todos'))\n",
    "dic_SVM=metrics.classification_report(y_test,predictSVM,target_names=['0.0','1.0','2.0'], output_dict=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea6e64f2409e8d1aad035b517afa5d7288b59435ef8e27ec4a88ca9998af4977"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
